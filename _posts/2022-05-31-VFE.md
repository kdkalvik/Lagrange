---
layout: post
title: "Tutorial on variational sparse Gaussian processes"
author: "Kalvik Jakkala"
categories: journal
tags: [documentation, sample]
abstract: "Walkthrough of the derivation of the variational sparse Gaussian processes [Titsias 2009]."
---

# Gaussian processes

[Gaussian processes](https://kdkalvik.github.io/GPs) are one of the most, if not the most, mathematically beautiful and elegant machine learning methods in history. We can use them for classification, regression, or generative problems. Also, the best part, they are probabilistic, so we can quantify the uncertainty in our predictions and have a lower risk of overfitting. 

Given a regression task's training set $$\mathcal{D} = \{(\mathbf{x}_i, y_i), i = 1,...,n\}$$ with $$n$$ data samples consisting of inputs $$\mathbf{x}_i \in \mathbb{R}^d$$ and noisy outputs $$y_i \in \mathbb{R}$$, we can use Gaussian processes to predict the noise free outputs $$f_*$$ (or noisy $$y_*$$) at test locations $$\mathbf{x}_*$$. The approach assumes that the relationship between the inputs  $$\mathbf{x}_i$$ and outputs  $$y_i$$ is given by

$$
y_i = f(\mathbf{x}_i) + \epsilon_i \quad \quad \text{where} \ \ \epsilon_i \sim \mathcal{N}(0, \sigma^2_{\text{noise}}) \\
$$

Here $$\sigma^2_{\text{noise}}$$ is the variance of the independent additive Gaussian noise in the observed outputs $$y_i$$. The latent function $$f(\mathbf{x})$$ models the noise free function of interest that explains the regression dataset. 

Gaussian processes (GP) model datasets formulated as shown above by assuming a GP prior over the space of functions that could be used to explain the dataset, i.e., they assume the following prior distribution over the function

$$
p(\mathbf{f} | \mathbf{X}) = \mathcal{N}(0, \mathbf{K}) \\
$$

where $$\mathbf{f} = [f_1, f_2,...,f_n]^\top$$ is a vector of latent function values, $$f_i = f(\mathbf{x_i})$$, $$\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2,...,\mathbf{x}_n]^\top$$ is a vector (or matrix) of inputs, and $$\mathbf{K} \in \mathbb{R}^{n \times n}$$ is a covariance matrix, whose entries $$\mathbf{K}_{ij}$$ are given by the kernel function $$k(x_i, x_j)$$. 

GPs use the kernel function to index and order the inputs $$\mathbf{x_i}$$ so that points closer to each other (i.e., have high covariance value from the kernel function) have similar labels and vice versa. The kernel function parameters are tuned using Type II maximum likelihood so that the GP accurately models the latent function of trainig data. Inference in GPs to get the output predictions $$\mathbf{y}$$, for the training set input samples $$\mathbf{X}$$, entails marginalizing the latent function values $$\mathbf{f}$$

$$
p(\mathbf{y, f} | \mathbf{X}) = p(\mathbf{y} | \mathbf{f}) p(\mathbf{f} | \mathbf{X}) \\
p(\mathbf{y} | \mathbf{X}) = \int p(\mathbf{y, f} | \mathbf{X}) d\mathbf{f}
$$

I will drop the explicit conditioning on the inputs $\mathbf{X}$ from here on to reduce the notational complexity and assume that the corresponding inputs are always in the conditioning set.

Inference on test points $$\mathbf{X_*}$$ to get the noise free predictions $$\mathbf{f}_*$$ (or noisy $$\mathbf{y}_*$$) can be done by considering the joint distribution over the training and test latent values, $$\mathbf{f}$$ and $$\mathbf{f}_*$$, and using Gaussian conditioning to marginalize the training set latent variables as shown below

$$
\begin{aligned}
p(\mathbf{f}, \mathbf{f}_* | \mathbf{y}) &= \frac{p(\mathbf{y} | \mathbf{f})p(\mathbf{f}, \mathbf{f}_*)}{p(\mathbf{y})} \\
p(\mathbf{f}_* | \mathbf{y}) &= \int \frac{p(\mathbf{y} | \mathbf{f})p(\mathbf{f}, \mathbf{f}_*)}{p(\mathbf{y})} d\mathbf{f} \\
&= \mathcal{N}(\mathbf{K}_{*f}(\mathbf{K}_{ff} + \sigma_{\text{noise}}^{2}I)^{-1}\mathbf{y}, \
              \mathbf{K}_{**}-\mathbf{K}_{*f}(\mathbf{K}_{ff} + \sigma_{\text{noise}}^{2}I)^{-1}\mathbf{K}_{f*})
\end{aligned}
$$

The problem with this approach is that it requires an inversion of a matrix of size $$n \times n$$, which is a $$\mathcal{O}(n^3)$$ operation, where $$n$$ is the number of training set samples. Thus this method can handle at most a few thousand training samples. Checkout my [tutorial on Gaussian processes](https://kdkalvik.github.io/GPs) for a comprehensive explanation. 

---

# Sparse Gaussian processes
#### Variational free energy (VFE) method [Titsias, 2009]

Sparse Gaussian processes (SGPs) address the computational cost issues of Gaussian processes. Although there are numerous SGP approaches, Titsias's variational free energy (VFE) method is the most well known approach and has had a significant impact on the Gaussian process literature. 

As the name suggests, VFE is a variational approach that we can use to find an approximate posterior distribution. The main idea behind variational methods is to pick a parametric family of distributions (the variational distribution) to model the variables of interest. The distribution is chosen so that it is computationally tractable compared to the true distribution of the variables. 

We use the evidence lower bound (ELBO) as the optimization objective that, when maximized, would result in a variational distribution close to the true distribution when optimized. The ELBO is derived as follows using Jensen's inequality on the log probability of the observed output variables $$\mathbf{y}$$

<details markdown=1>
  <summary>Click for details on Jensen's inequality</summary>  
  Jensen's inequality states that the following holds for convex functions

  $$
  f(\mathbb{E}[\mathbf{X}]) \geq \mathbb{E}[f(\mathbf{X})]
  $$

  The $\log$ operation is a convex function. You can learn more about Jensen's inequality [here](https://en.wikipedia.org/wiki/Jensen%27s_inequality).

</details>

$$
\begin{aligned}
\log p(\mathbf{y}) &= \log \int p(\mathbf{y}, \mathbf{f}) d\mathbf{f} \\
&= \log \int p(\mathbf{y}, \mathbf{f}) \frac{q(\mathbf{f})}{q(\mathbf{f})} d\mathbf{f} \\
&= \log \int q(\mathbf{f}) \frac{p(\mathbf{y}, \mathbf{f})}{q(\mathbf{f})} d\mathbf{f} \\
&\geq \int q(\mathbf{f}) \log \frac{p(\mathbf{y, f})}{q(\mathbf{f})} d\mathbf{f} \ \ \ \text{(from Jensen's inequality)}\\
&= \mathbb{E}_q[\log p(\mathbf{y, f})] - \mathbb{E}_q[\log q(\mathbf{f})] \\
&= \mathcal{F}(q)
\end{aligned}
$$

Here $$q(\mathbf{f})$$ is the variational distribution. We choose a family of variational distributions such that the expectations are computationally efficient (refer to Bishop's [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) book, which has a chapter on this topic for more details).

The bound $$\mathcal{F}(q)$$ can also be written as the difference between the model log-marginal and the KL divergence between the variational distribution and the true posterior 

$$
\mathcal{F}(q) = \log p(\mathbf{y}) - \text{KL}(q(\mathbf{f}) || p(\mathbf{f|y}))
$$

<details markdown=1>
  <summary>This follows from an alternate form of the ELBO (click for details)</summary>  

  From the product rule we know
  $$
  p(\mathbf{f} | \mathbf{y}) = \frac{p(\mathbf{f}, \mathbf{y})}{p(\mathbf{y})}
  $$

  Then using KL divergence we get the following

  $$
  \begin{aligned}
  \text{KL}(q(\mathbf{f}) || p(\mathbf{f|y})) &= \mathbb{E}_q \left[ \log \frac{q({\mathbf{f}})}{p({\mathbf{f|y}})} \right] \\
  &= \mathbb{E}_q \left[ \log q({\mathbf{f}}) \right] - \mathbb{E}_q \left[ \log p({\mathbf{f|y}}) \right] \\
  &= \mathbb{E}_q \left[ \log q({\mathbf{f}}) \right] - \mathbb{E}_q \left[ \log p({\mathbf{f, y}}) \right] + \log p({\mathbf{y}}) \\
  &= -(\mathbb{E}_q \left[ \log p({\mathbf{f, y}}) \right] - \mathbb{E}_q \left[ \log q({\mathbf{f}}) \right]) + \log p({\mathbf{y}}) \\
  \end{aligned}
  $$

  This is the negative ELBO plus the log marginal probability of $\mathbf{y}$.
</details>

Therefore, the above bound $$\mathcal{F}(q)$$ will be tight when the variational distribution $$q(\mathbf{f})$$ is equal to the conditional distribution $$p(\mathbf{f} \mid \mathbf{y})$$. This is because when the following equality holds, the $$\text{KL}$$ term in the bound $$\mathcal{F}(q)$$ vanishes and the log-marginal is independent of the variational distribution $$q(\mathbf{f})$$. 

$$
q(\mathbf{f}) = p(\mathbf{f|y})
$$

But such a variational distribution is intractable. Therefore, we instead introduce explicit pseudo-points $$\mathbf{X}_u$$ with latent variables $$\mathbf{u}$$ such that the set of latent variables is now defined as follows

$$
\mathbf{f} = \{ \mathbf{u}, \mathbf{f}_{\neq \mathbf{u}} \} \\
$$

Here the latent variables $$\mathbf{f}_{\neq \mathbf{u}}$$ correspond to the training set inputs $$\mathbf{X}$$. Using the updated latent variables $$\mathbf{f}$$, we can factorize the variational distribution $$q(\mathbf{f})$$ as follows

$$
\begin{aligned}
q(\mathbf{f}) &= q(\mathbf{f}_{\neq \mathbf{u}}, \mathbf{u}) \\
              &= p(\mathbf{f}_{\neq \mathbf{u}} | \mathbf{u}) q(\mathbf{u})
\end{aligned}
$$

The above variational distribution is the essence of sparse Gaussian processes. It assumes that the latent variables $$\mathbf{f}_{\neq \mathbf{u}}$$ which correspond to the training set inputs $$\mathbf{X}$$ are independent of the training set outputs $$\mathbf{y}$$ given the inducing point latent variables $$\mathbf{u}$$. Now contrast the above variational distribution factorization with the true posterior distribution's factorization shown below

$$
p(\mathbf{f}|\mathbf{y}) = p(\mathbf{f}_{\neq \mathbf{u}} | \mathbf{y}, \mathbf{u}) p(\mathbf{u|y}) \\
$$

Our variational distribution $$q(\mathbf{u})$$ is optimized using the training data, but it isn't explicitly conditioned on the training set outputs $$\mathbf{y}$$, and similarly, the conditional $$p(\mathbf{f}_{\neq \mathbf{u}} \mid \mathbf{u})$$ used in $$q(\mathbf{f})$$ is independent of the training set outputs $$\mathbf{y}$$ given the latent variables $$\mathbf{u}$$. 

Also note that even though the conditional $$p(\mathbf{f}_{\neq \mathbf{u}} \mid \mathbf{u})$$ is not the same as the corresponding conditional $$p(\mathbf{f}_{\neq \mathbf{u}} \mid \mathbf{y}, \mathbf{u})$$ used in $$p(\mathbf{f} \mid \mathbf{y})$$, it is still not a variational distribution as it can be calculated using Gaussian conditioning. 

Now that we have a variationl distribution, we can substute it into the bound $$\mathcal{F}(q)$$. 

$$
\begin{aligned}
\require{cancel}
\mathcal{F}(q) &= \int q(\mathbf{f}) \log \frac{p(\mathbf{y, f})}{q(\mathbf{f})} d\mathbf{f} \\
&= \int p(\mathbf{f}_{\neq \mathbf{u}} | \mathbf{u}) q(\mathbf{u}) \log \frac{p(\mathbf{y|f}) \cancel{p(\mathbf{f}_{\neq \mathbf{u}} | \mathbf{u})} p(\mathbf{u})}{\cancel{p(\mathbf{f}_{\neq \mathbf{u}} | \mathbf{u})} q(\mathbf{u})} d\mathbf{f} \\
&= \int p(\mathbf{f}_{\neq \mathbf{u}} | \mathbf{u}) q(\mathbf{u}) \log \frac{p(\mathbf{y|f}) p(\mathbf{u})}{q(\mathbf{u})} d\mathbf{f}
\end{aligned}
$$

Here we were able to factorize the joint distribution $$p(\mathbf{y, f})$$ as shown above only because of the assumption we made about $$\mathbf{f}$$ being independent of $$\mathbf{y}$$ given $$\mathbf{u}$$. I will drop the "$$\neq \mathbf{u}$$" subscript for $$\mathbf{f}$$ from here on to keep the notation clean. 

Note that in the above bound, we need to know the latent variables $$\mathbf{u}$$ to evaluate it. One approach would be to select a subset of the training set inputs $$\mathbf{X}$$ and use them as the inducing point inputs $$\mathbf{X}_u$$, which can then be passed to the kernel function to obtain the distribution over the latent variables $$\mathbf{u}$$. This approach is called the Subset of Data (SoD) approximation, but it is far too naive as we could pick a subset that is not a good representative of the whole training dataset. Another approach is to use maximum likelihood to optimize the inducing point inputs, which is the Fully Independent Training Conditional (FITC) Approximation. But it is prone to overfitting. 

Therefore, Titsias integrated the distribution over the latent variables $$\mathbf{u}$$. The factorization of the variational distribution allows us to compute the integral analytically. 

$$
\begin{aligned}
\mathcal{F}(q) &= \int p(\mathbf{f|u}) q(\mathbf{u}) \log \frac{p(\mathbf{y|f}) p(\mathbf{u})}{q(\mathbf{u})} d\mathbf{f} d\mathbf{u} \\
&= \int q(\mathbf{u}) \left( \int p(\mathbf{f|u})  \log \frac{p(\mathbf{y|f}) p(\mathbf{u})}{q(\mathbf{u})} d\mathbf{f} \right) d\mathbf{u} \\
&= \int q(\mathbf{u}) \left( \underbrace{\int p(\mathbf{f|u})  \log p(\mathbf{y|f}) d\mathbf{f}}_{G(\mathbf{u, y})} + \log \frac{p(\mathbf{u})}{q(\mathbf{u})} \right) d\mathbf{u}
\end{aligned}
$$

The inner integral over $$\mathbf{f}$$ can be computed as follows

$$
\begin{aligned}
G(\mathbf{u, y}) &= \int p(\mathbf{f|u})  \log p(\mathbf{y|f}) d\mathbf{f} \\
&= \int p(\mathbf{f|u}) \left( -\frac{n}{2} \log(2\pi \sigma^2) - \frac{1}{2 \sigma^2} Tr \left[ \mathbf{y}\mathbf{y}^\top - 2 \mathbf{y} \mathbf{f}^\top + \mathbf{f}\mathbf{f}^\top \right] \right) d\mathbf{f} \\
&= -\frac{n}{2} \log(2\pi \sigma^2) - \frac{1}{2 \sigma^2} Tr \left[ \mathbf{y}\mathbf{y}^\top - 2 \mathbf{y} \boldsymbol{\alpha}^\top + \boldsymbol{\alpha}\boldsymbol{\alpha}^\top + \mathbf{K}_{ff} - \mathbf{Q} \right]  \\
&= \log [\mathcal{N}(\mathbf{y} | \boldsymbol{\alpha}, \sigma^2I)]  - \frac{1}{2 \sigma^2} Tr (\mathbf{K}_{ff} - \mathbf{Q})  \\
\end{aligned}
$$

<details>
  <summary>Follows from this equation</summary>
  $$
  \text{cov}[\mathbf{f}|\mathbf{u}] = \mathbb{E}[\mathbf{f}\mathbf{f}^\top|\mathbf{u}] - \mathbb{E}[\mathbf{f}|\mathbf{u}]\mathbb{E}[\mathbf{f}|\mathbf{u}]^\top \\
  \mathbb{E}[\mathbf{f}\mathbf{f}^\top|\mathbf{u}] = \mathbb{E}[\mathbf{f}|\mathbf{u}]\mathbb{E}[\mathbf{f}|\mathbf{u}]^\top + \text{cov}[\mathbf{f}|\mathbf{u}]
  $$
  Also, because the trace operation is linear.
</details>

Here,
 
$$
\begin{aligned}
\boldsymbol{\alpha} &= \mathbb{E}[\mathbf{f} \mid \mathbf{u}] = \mathbf{K}_{fu}\mathbf{K}_{uu}^{-1}\mathbf{u} \\
\mathbf{Q} &= \mathbf{K}_{fu}\mathbf{K}_{uu}^{-1}\mathbf{K}_{uf} \\
\end{aligned}
$$

And $$\mathbf{K}$$ is the kernel matrix with the subscript denoting the variables used to compute the matrix.


Plugging $$G(\mathbf{u, y})$$ back into the bound $$\mathcal{F}(q)$$, we get the following

$$
\begin{aligned}
\mathcal{F}(q) &= \int q(\mathbf{u}) \left( \log[\mathcal{N}(\mathbf{y} | \boldsymbol{\alpha}, \sigma^2I)]  - \frac{1}{2 \sigma^2} Tr (\mathbf{K}_{ff} - \mathbf{Q}) + \log \frac{p(\mathbf{u})}{q(\mathbf{u})} \right) d\mathbf{u}\\
&= \int q(\mathbf{u}) \left( \log[\mathcal{N}(\mathbf{y} | \boldsymbol{\alpha}, \sigma^2I)] + \log \frac{p(\mathbf{u})}{q(\mathbf{u})} \right) d\mathbf{u} - \underbrace{\frac{1}{2 \sigma^2} Tr (\mathbf{K}_{ff} - \mathbf{Q})}_{\text{independent of } \mathbf{u}}\\
&= \int q(\mathbf{u}) \log \frac{\mathcal{N}(\mathbf{y} | \boldsymbol{\alpha}, \sigma^2I) p(\mathbf{u})}{q(\mathbf{u})} d\mathbf{u} - \frac{1}{2 \sigma^2} Tr (\mathbf{K}_{ff} - \mathbf{Q}) \\
\end{aligned}
$$

If we reverse the Jensen's inequality in the bound $$\mathcal{F}(q)$$, it would convert the inequality in the ELBO to an equality. This would give us the optimal bound, which is achieved with the optimal variational distribution $$q^*(\mathbf{u})$$

$$
\begin{aligned}
\require{cancel}
\mathcal{F}^*(q) &= \log \int \cancel{q(\mathbf{u})} \frac{\mathcal{N}(\mathbf{y} | \boldsymbol{\alpha}, \sigma^2I) p(\mathbf{u})}{\cancel{q(\mathbf{u})}} d\mathbf{u} - \frac{1}{2 \sigma^2} Tr (\mathbf{K}_{ff} - \mathbf{Q}) \\
&= \log \int \mathcal{N}(\mathbf{y} | \boldsymbol{\alpha}, \sigma^2I) p(\mathbf{u}) d\mathbf{u} - \frac{1}{2 \sigma^2} Tr (\mathbf{K}_{ff} - \mathbf{Q}) \\
&= \log [\mathcal{N}(\mathbf{y} | 0, \sigma^2I + \mathbf{Q})] - \frac{1}{2 \sigma^2} Tr (\mathbf{K}_{ff} - \mathbf{Q}) \\
\end{aligned}
$$

From the above equation, we know that the optimal distribution $$q^*(\mathbf{u})$$ that gives us the optimal bound is given by
$$
\begin{aligned}
q^*(\mathbf{u}) &\propto \mathcal{N}(\mathbf{y} | \boldsymbol{\alpha}, \sigma^2I) p(\mathbf{u}) \\
&= c \exp{ \left( \frac{1}{2} \mathbf{u}^\top (\mathbf{K}_{uu}^{-1} + \frac{1}{2 \sigma^2} \mathbf{K}_{uu}^{-1}\mathbf{K}_{uf}\mathbf{K}_{fu}\mathbf{K}_{uu}^{-1}  ) \mathbf{u} + \frac{1}{2 \sigma^2} \mathbf{y}^\top \mathbf{K}_{uf} \mathbf{K}_{uu}^{-1} \mathbf{u} \right)}
\end{aligned}
$$

where $$c$$ is a constant. Completing the quadratic form we recognize the Gaussian

$$
q^*(\mathbf{u}) = \mathcal{N}(\mathbf{u} | \sigma^{-2} \mathbf{K}_{uu} \mathbf{\Sigma}^{-1} \mathbf{K}_{uf} \mathbf{y}, \mathbf{K}_{uu} \mathbf{\Sigma}^{-1} \mathbf{K}_{uu})
$$

Where $$\mathbf{\Sigma} = \mathbf{K}_{uu} + \sigma^{-2}\mathbf{K}_{uf}\mathbf{K}_{fu}$$

And **voila!** The above optimal variational distribution can be computed analytically given the training dataset. All that remains is to figure out how to compute the test set labels once the above variational distribution over the inducing variables is learned. We can do that by considering the following factorization

$$
p(\mathbf{f}_* | \mathbf{y}) = \int p(\mathbf{f}_* \mid \mathbf{u}, \mathbf{f}) p(\mathbf{f} \mid \mathbf{y}, \mathbf{u}) p(\mathbf{u} | \mathbf{y}) d\mathbf{f} d\mathbf{u} \\
$$

The above is the factorization of the exact distribution. Now assuming that the inducing variables $\mathbf{u}$ are a sufficient statistic for the training variables $\mathbf{f}$, i.e., $p(\mathbf{f} \| \mathbf{y}, \mathbf{u}) = p(\mathbf{f} \| \mathbf{u})$, which would also imply $$p(\mathbf{f}_* \mid \mathbf{u}, \mathbf{f}) = p(\mathbf{f}_* \mid \mathbf{u})$$, we get the following 

$$
\begin{aligned}
p(\mathbf{f}_* | \mathbf{y}) &= \int p(\mathbf{f}_* | \mathbf{u}) p(\mathbf{f} | \mathbf{u}) p(\mathbf{u} | \mathbf{y}) d\mathbf{f} d\mathbf{u} \\
&= \int p(\mathbf{f}_* | \mathbf{u}) p(\mathbf{u} | \mathbf{y}) d\mathbf{u} \int p(\mathbf{f} | \mathbf{u}) d\mathbf{f} \\
&= \int p(\mathbf{f}_* | \mathbf{u}) p(\mathbf{u} | \mathbf{y}) d\mathbf{u}
\end{aligned}
$$

Replacing $$p(\mathbf{u} \mid y)$$ in the above integral with the optimal variational distribution over the inducing points $$q^*(\mathbf{u})$$, it evaluates to a Gaussian with the following mean and covariance

$$
m(x) = \mathbf{K}_{xu} \mathbf{K}_{uu}^{-1} \boldsymbol{\mu} \\
k(x, x^\prime) = k(x, x^\prime) - \mathbf{K}_{xu} \mathbf{K}_{uu}^{-1} \mathbf{K}_{ux^\prime} + \mathbf{K}_{xu} \mathbf{K}_{uu}^{-1} \mathbf{A} \mathbf{K}_{uu}^{-1} \mathbf{K}_{ux^\prime} \\
$$

Where, $$\boldsymbol{\mu}$$ and $$ \mathbf{A}$$ are the mean and covariance of the variational distribution $$q^*(\mathbf{u})$$ we derived above. 

---

# Appendix

The following are some important equations used in the derivation above

### Gaussian margnial and condititonal distributions

If we have a marginal Gaussian distribution for $\mathbf{u}$ and a conditional Gaussian distribution for $\mathbf{f}$ given $\mathbf{u}$ as shown below, the marginal distribution of $\mathbf{f}$ is given as follows

$$
\begin{aligned}
p(\mathbf{u}) &= \mathcal{N}(\mathbf{u} | \mathbf{\mu}_u, \mathbf{\Sigma}_u) \\
p(\mathbf{f|u}) &= \mathcal{N}(\mathbf{f} | \mathbf{Mu+m}, \mathbf{\Sigma}_f) \\
p(\mathbf{f}) &= \mathcal{N}(\mathbf{f} | \mathbf{M}\mathbf{\mu}_u + \mathbf{m}, \mathbf{\Sigma}_f + \mathbf{M}\mathbf{\Sigma}_u \mathbf{M}^\top)
\end{aligned} \tag{A.1}
$$

### Woodbury matrix identity
Given an invertable matrix $\mathbf{A}$ of size $n \times n$, matrices $\mathbf{U}$ and $\mathbf{V}$ of size $n \times m$, and an invertable matrix $\mathbf{W}$ of size $m \times m$

$$
(\mathbf{A} + \mathbf{U}\mathbf{W}\mathbf{V}^\top)^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1}\mathbf{U}(\mathbf{W}^{-1} + \mathbf{V}^\top \mathbf{A}^{-1} \mathbf{U})^{-1} \mathbf{V}^\top \mathbf{A}^{-1} \tag{A.2}
$$

### Matrix determinant lemma

$$
|\mathbf{A} + \mathbf{U} \mathbf{W} \mathbf{V}^\top| = |\mathbf{W}^{-1} + \mathbf{V}^\top \mathbf{A}^{-1} \mathbf{U}| |\mathbf{W}| |\mathbf{A}| \tag{A.3}
$$

where $\|.\|$ denotes the determinant of a matrix.

---

# References

* [Titsia's original paper](http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf)
* [Titsia's derivation of VFE](https://www2.aueb.gr/users/mtitsias/papers/sparseGPv2.pdf)
* [ELBO derivation](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)
* [Thang Bui and Richard Turner's notes on VFE](http://mlg.eng.cam.ac.uk/thang/docs/talks/rcc_vargp.pdf)
* [Bauer et al. paper which studied VFE and how it compares to FITC](https://proceedings.neurips.cc/paper/2016/file/7250eb93b3c18cc9daa29cf58af7a004-Paper.pdf)
* [FITC paper (anothes popular SGP approach)](https://papers.nips.cc/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf)
* [Matthews et al. paper which showed the KL divergence intrepretation of the VFE derivation](https://arxiv.org/pdf/1504.07027.pdf)
* [Hensman et al. paper which generalized the approach to stochastic variational inference. It allows us to scale the approach to large datasets with millions of data points and non-Gaussian likelihoods](https://arxiv.org/ftp/arxiv/papers/1309/1309.6835.pdf)
* [Andreas Damianou's PhD thesis, really good resource for GPs, SGPs, and deep GPs](https://etheses.whiterose.ac.uk/9968/1/Damianou_Thesis.pdf)
* [Another cool VFE walkthrough](https://tiao.io/post/sparse-variational-gaussian-processes/)
* [A recent survey/approach on SGPs](https://arxiv.org/pdf/1605.07066.pdf)
* [The OG survey on SGPs that mentions FITC and SoD](https://www.jmlr.org/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf)